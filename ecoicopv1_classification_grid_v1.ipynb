{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring ECOICOP v1 information for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ECOICOP v1 classification contains unstructured information for its most detailed level of classification. This notebook leverages LLMs in order to structure this information in a format which can be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time # Add delays because of free API rate limits\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.mistral import MistralModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio # Fix issues with Jupyter notebook event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import environment variables with API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Pydantic model for parsing the additional information in each ECOICOP level 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcoicopDetails(BaseModel):\n",
    "    examples: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read ECOICOP definitions (only selected columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = [\"CODE\", \"NAME_EN\", \"LEVEL\", \"PARENT_CODE\", \"InclusionNote\"]\n",
    "\n",
    "data_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"ecoicop_v1/ECOICOP-HICP_Structure_Labels.tsv\", \n",
    "    sep=\"\\t\",\n",
    "    usecols=usecols,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only level 4 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.loc[data_df[\"LEVEL\"] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize PydanticAI agent to structure information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"mistral-large-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralModel(model_name=llm_model, api_key=config.get(\"MISTRAL_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=EcoicopDetails,\n",
    "    system_prompt=(\n",
    "        'You are an expert data curator. You will receive a string of text '\n",
    "        'with examples of items to be included into a certain classification. '\n",
    "        'Your task is to transform this text into a list of self-explainig '\n",
    "        'items descriptions, exploding the examples to the most granular level you can '\n",
    "        'identify in the text. '\n",
    "        'Each item description should contain all necessary information for classification '\n",
    "        'as provided in the original text. Prefer complete descriptions rather than single words. '\n",
    "        'If possible, avoid the use of ambiguous or generic terms such as `other` or `miscellaneous`. '\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run calls to the Agent to extract and format information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"InclusionNote\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = \"{}. {}\".format(item.get(\"NAME_EN\"), item.get(\"InclusionNote\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"NAME_EN\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"NAME_EN\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"NAME_EN\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and failed calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"InclusionNote\", \"LEVEL\"]).to_csv(\n",
    "    \"results/ecoicop_v1_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_ecoicop_v1_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.3 on Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GROQ_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=EcoicopDetails,\n",
    "    system_prompt=(\n",
    "        'You are an expert data curator. You will receive a string of text '\n",
    "        'with examples of items to be included into a certain classification. '\n",
    "        'Your task is to transform this text into a list of self-explainig '\n",
    "        'items descriptions, exploding the examples to the most granular level you can '\n",
    "        'identify in the text. '\n",
    "        'Each item description should contain all necessary information for classification '\n",
    "        'as provided in the original text. Prefer complete descriptions rather than single words. '\n",
    "        'If possible, avoid the use of ambiguous or generic terms such as `other` or `miscellaneous`. '\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"InclusionNote\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = \"{}. {}\".format(item.get(\"NAME_EN\"), item.get(\"InclusionNote\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"NAME_EN\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"NAME_EN\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"NAME_EN\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"InclusionNote\", \"LEVEL\"]).to_csv(\n",
    "    \"results/ecoicop_v1_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_ecoicop_v1_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek on Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"deepseek-r1-distill-llama-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GROQ_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=EcoicopDetails,\n",
    "    system_prompt=(\n",
    "        'You are an expert data curator. You will receive a string of text '\n",
    "        'with examples of items to be included into a certain classification. '\n",
    "        'Your task is to transform this text into a list of self-explainig '\n",
    "        'items descriptions, exploding the examples to the most granular level you can '\n",
    "        'identify in the text. '\n",
    "        'Each item description should contain all necessary information for classification '\n",
    "        'as provided in the original text. Prefer complete descriptions rather than single words. '\n",
    "        'If possible, avoid the use of ambiguous or generic terms such as `other` or `miscellaneous`. '\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"InclusionNote\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = \"{}. {}\".format(item.get(\"NAME_EN\"), item.get(\"InclusionNote\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"NAME_EN\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"NAME_EN\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"NAME_EN\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"InclusionNote\", \"LEVEL\"]).to_csv(\n",
    "    \"results/ecoicop_v1_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_ecoicop_v1_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = [f for f in os.listdir(\"results/\") if f.startswith(\"ecoicop_v1\") and f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for f in result_files:\n",
    "    temp_df = pd.read_csv(os.path.join(\"results\", f))\n",
    "    results_list.append(temp_df)\n",
    "\n",
    "results_df = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to lowercase and remove all \"other\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "results_df[\"NAME_EN\"] = results_df[\"NAME_EN\"].str.lower()\n",
    "# Remove duplicates\n",
    "results_df = results_df.drop_duplicates(ignore_index=True)\n",
    "# Remove items with \"other\" or \"miscellaneous\"\n",
    "results_df = results_df[~results_df[\"NAME_EN\"].str.contains(\"other|miscellaneous\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\n",
    "    \"results/consolidated_ecoicop_v1_{}.csv\".format(datetime.now().strftime(\"%Y-%m-%d\")), \n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
