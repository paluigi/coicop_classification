{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring COICOP 2018 information for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COICOP 2018 classification contains unstructured information for its most detailed level of classification. This notebook leverages LLMs in order to structure this information in a format which can be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time # Add delays because of free API rate limits\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "from unidecode import unidecode\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.mistral import MistralModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio # Fix issues with Jupyter notebook event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import environment variables with API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Pydantic model for parsing the additional information in each COICOP 2018 level 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoicopDetails(BaseModel):\n",
    "    examples: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read COICOP 2018 definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = [\"code\",\"title\",\"intro\", \"includes\", \"alsoIncludes\", \"excludes\"]\n",
    "\n",
    "data_df = pd.read_excel(\n",
    "    \"coicop_2018/COICOP_2018_English_structure_edited.xlsx\", \n",
    "    usecols=usecols,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove exclusion note from the English and French files, rename Spanish columns.\n",
    "\n",
    "Remove (ND), (SD), (D), (S) markings from the class names\n",
    "\n",
    "Filter level 4 classes only for LLMs queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subclasses(df):\n",
    "    \"\"\"\n",
    "    Process DataFrame with the following operations:\n",
    "    1. Filter rows where Code contains exactly 3 dots\n",
    "    2. Combine description columns\n",
    "    3. Remove classification markers from Description\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # 1. Select rows where Code has exactly 2 dots\n",
    "    result_df = result_df[result_df['code'].str.count(r'\\.') == 3]\n",
    "    \n",
    "    # 2. Combine description columns\n",
    "    columns_to_concat = [\"intro\", \"includes\", \"alsoIncludes\"]\n",
    "    \n",
    "    def concatenate_non_nan_columns(row):\n",
    "        # Filter out NaN values and convert to string\n",
    "        non_nan_values = [str(row[col]) for col in columns_to_concat if pd.notna(row[col])]\n",
    "        return ' \\n '.join(non_nan_values) if non_nan_values else ''\n",
    "    \n",
    "    result_df['description'] = result_df.apply(concatenate_non_nan_columns, axis=1)\n",
    "    # Fix encoding issues\n",
    "    result_df['description'] = result_df['description'].apply(fix_text)\n",
    "    result_df['description'] = result_df['description'].apply(unidecode)\n",
    "    result_df['description'] = result_df['description'].str.replace(\"_x000D_\", \" \")\n",
    "\n",
    "    # Fix the exclusion column\n",
    "    result_df['excludes'] = result_df['excludes'].fillna('')\n",
    "    result_df['excludes'] = result_df['excludes'].apply(fix_text)\n",
    "    result_df['excludes'] = result_df['excludes'].apply(unidecode)\n",
    "    result_df['excludes'] = result_df['excludes'].str.replace(\"_x000D_\", \" \")\n",
    "        \n",
    "    # 3. Remove classification markers from Description\n",
    "    markers_pattern = r'\\s*\\((ND|SD|S|D)\\)'\n",
    "    result_df['title'] = result_df['title'].str.replace(markers_pattern, '', regex=True)\n",
    "    \n",
    "    return result_df[[\"code\", \"title\", \"description\", \"excludes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subsubclasses(df):\n",
    "    \"\"\"\n",
    "    Process DataFrame with the following operations:\n",
    "    1. Filter rows where Code contains exactly 4 dots\n",
    "    2. Combine description columns\n",
    "    3. Remove classification markers from Description\n",
    "    4. censor the code to subclass level\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # 1. Select rows where Code has exactly 3 dots\n",
    "    result_df = result_df[result_df['code'].str.count(r'\\.') == 4]\n",
    "    \n",
    "    # 2. Combine description columns\n",
    "    columns_to_concat = [\"intro\", \"includes\", \"alsoIncludes\"]\n",
    "    \n",
    "    def concatenate_non_nan_columns(row):\n",
    "        # Filter out NaN values and convert to string\n",
    "        non_nan_values = [str(row[col]) for col in columns_to_concat if pd.notna(row[col])]\n",
    "        return ' \\n '.join(non_nan_values) if non_nan_values else ''\n",
    "    \n",
    "    result_df['description'] = result_df.apply(concatenate_non_nan_columns, axis=1)\n",
    "    # Fix encoding issues\n",
    "    result_df['description'] = result_df['description'].apply(fix_text)\n",
    "    result_df['description'] = result_df['description'].apply(unidecode)\n",
    "    result_df['description'] = result_df['description'].str.replace(\"_x000D_\", \" \")\n",
    "\n",
    "    # Fix the exclusion column\n",
    "    result_df['excludes'] = result_df['excludes'].fillna('')\n",
    "    result_df['excludes'] = result_df['excludes'].apply(fix_text)\n",
    "    result_df['excludes'] = result_df['excludes'].apply(unidecode)\n",
    "    result_df['excludes'] = result_df['excludes'].str.replace(\"_x000D_\", \" \")\n",
    "        \n",
    "    # 3. Remove classification markers from Description\n",
    "    markers_pattern = r'\\s*\\((ND|SD|S|D)\\)'\n",
    "    result_df['title'] = result_df['title'].str.replace(markers_pattern, '', regex=True)\n",
    "\n",
    "    # 4. censor the code to subclass level\n",
    "    result_df['code'] = result_df['code'].str[:8]\n",
    "    \n",
    "    return result_df[[\"code\", \"title\", \"description\", \"excludes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsub_df = process_subsubclasses(data_df)\n",
    "sub_df = process_subclasses(data_df)\n",
    "data_df = pd.concat([sub_df, subsub_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove divisions 14 and 15, as they do not add further information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[~data_df[\"code\"].str.startswith((\"14\", \"15\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative versions\n",
    "# system_prompt = \"\"\"You are an expert data curator specializing in semantic distinctiveness. When provided with a subclass name and its inclusion/exclusion description:\n",
    "\n",
    "# 1. Parse the inclusion description to identify all distinct items that belong in the subclass, at the most granular level possible.\n",
    "#    - For example, if the text says “Chocolate bars,” specify types (milk chocolate, dark chocolate, etc.)\n",
    "#    - Use the exclusion description only to clarify boundaries.\n",
    "\n",
    "# 2. For each included item, generate a distinctive description that:\n",
    "#    - Incorporates the subclass context while focusing on item-specific attributes\n",
    "#    - Emphasizes unique characteristics that differentiate it from other items\n",
    "#    - Uses precise terminology relevant to the domain of the subclass\n",
    "#    - Maximizes semantic distance between item descriptions\n",
    "\n",
    "# 3. Ensure each item description:\n",
    "#    - Avoids generic placeholder terms (like \"other,\" \"miscellaneous,\" \"various,\" \"assorted,\" \"etc.\")\n",
    "#    - Eliminates redundant phrasing and filler words\n",
    "#    - Uses synonyms and alternative phrasing to minimize lexical overlap between descriptions\n",
    "#    - Maintains specificity without resorting to catch-all language\n",
    "\n",
    "# 4. Produce output in the same language as the input text, preserving technical terminology while maximizing semantic uniqueness across all descriptions.\n",
    "\n",
    "# 5. Format each output entry to include the semantically distinctive item description for included items only\n",
    "# \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"ou are an expert data curator. When provided with a subclass name and its inclusion/exclusion description:\n",
    "\n",
    "1. Identify specific products/services that belong in this subclass based on the inclusion description. Use exclusions only to understand boundaries.\n",
    "\n",
    "2. Generate a list of specific product/service names that:\n",
    "   - Belong within the defined subclass\n",
    "   - Represent specific items, not categories\n",
    "   - Cover the full range of inclusions\n",
    "\n",
    "3. Make each name semantically unique by:\n",
    "   - Avoiding generic terms (\"other,\" \"miscellaneous\")\n",
    "   - Minimizing word overlap between entries\n",
    "   - Using varied terminology\n",
    "\n",
    "4. Output in the same language as input, preserving industry terminology.\n",
    "\n",
    "5. Format as a simple list of product/service names.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subclass_prompt = \"\"\"Subclass title: {title}\n",
    "Inclusion note: {description}\n",
    "Exclusion note: {excludes}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize PydanticAI agent to structure information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"mistral-large-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralModel(model_name=llm_model, api_key=config.get(\"MISTRAL_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run calls to the Agent to extract and format information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = subclass_prompt.format(\n",
    "            title=item.get(\"title\"),\n",
    "            description=item.get(\"description\"), \n",
    "            excludes=item.get(\"excludes\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and failed calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3 on Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"llama3-70b-8192\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GROQ_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = subclass_prompt.format(\n",
    "            title=item.get(\"title\"),\n",
    "            description=item.get(\"description\"), \n",
    "            excludes=item.get(\"excludes\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral on Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"mixtral-8x7b-32768\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GROQ_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = subclass_prompt.format(\n",
    "            title=item.get(\"title\"),\n",
    "            description=item.get(\"description\"), \n",
    "            excludes=item.get(\"excludes\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini 2.0 Flash Lite Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gemini-2.0-flash-lite-preview-02-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeminiModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GEMINI_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(4)\n",
    "    try:\n",
    "        prompt = subclass_prompt.format(\n",
    "            title=item.get(\"title\"),\n",
    "            description=item.get(\"description\"), \n",
    "            excludes=item.get(\"excludes\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = [f for f in os.listdir(\"results/\") if f.startswith(\"coicop2018\") and f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for f in result_files:\n",
    "    temp_df = pd.read_csv(os.path.join(\"results\", f))\n",
    "    results_list.append(temp_df)\n",
    "\n",
    "results_df = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"code\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to lowercase and remove all \"other\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "results_df[\"title\"] = results_df[\"title\"].str.lower()\n",
    "# Remove duplicates\n",
    "results_df = results_df.drop_duplicates(ignore_index=True)\n",
    "# Remove NAs\n",
    "results_df = results_df.dropna(subset=[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove items with \"other\" or \"miscellaneous\"\n",
    "results_df = results_df[~(results_df[\"title\"].str.contains(pat=\"other|miscellaneous\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models may have generated the same label with reference to different COICOP subclasses. Temporarily, we will simply drops the duplicates keeping the first occurrence. Possibly, we could process those occurrences with a LLM in order to keep the most suitable one according to the COICOP definitions of each subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.drop_duplicates(subset=[\"title\"], keep=\"first\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"code\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\n",
    "    \"results/consolidated_coicop2018_{}.csv\".format(\n",
    "        datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    ), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
