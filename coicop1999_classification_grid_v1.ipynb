{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring COICOP 1999 information for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COICOPP 1999 classification contains unstructured information for its most detailed level of classification. This notebook leverages LLMs in order to structure this information in a format which can be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time # Add delays because of free API rate limits\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.mistral import MistralModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio # Fix issues with Jupyter notebook event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import environment variables with API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Pydantic model for parsing the additional information in each COICOP 1999 level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoicopDetails(BaseModel):\n",
    "    examples: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read COICOP 1999 definitions (different languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = [\"Code\",\"Description\",\"ExplanatoryNote\"]\n",
    "usecols_es = [\"Code\",\"Title\",\"ExplanatoryNoteInclusion\"] # Spanish has different columns\n",
    "\n",
    "data_en = pd.read_csv(\n",
    "    filepath_or_buffer=\"coicop_1999/coicop1999_en.csv\", \n",
    "    usecols=usecols,)\n",
    "\n",
    "data_fr = pd.read_csv(\n",
    "    filepath_or_buffer=\"coicop_1999/coicop1999_fr.csv\", \n",
    "    usecols=usecols,)\n",
    "\n",
    "data_es = pd.read_csv(\n",
    "    filepath_or_buffer=\"coicop_1999/coicop1999_es.csv\", \n",
    "    usecols=usecols_es,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove exclusion note from the English and French files, rename Spanish columns.\n",
    "\n",
    "Remove (ND), (SD), (D), (S) markings from the class names\n",
    "\n",
    "Filter level 3 classes only for LLMs queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_classes(df):\n",
    "    \"\"\"\n",
    "    Process DataFrame with the following operations:\n",
    "    1. Filter rows where Code contains exactly 2 dots\n",
    "    2. Truncate ExplanatoryNote at either 'Excludes:' or 'Sont exclus'\n",
    "    3. Remove classification markers from Description\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with columns Code, Description, and ExplanatoryNote\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # 1. Select rows where Code has exactly 2 dots\n",
    "    result_df = result_df[result_df['Code'].str.count(r'\\.') == 2]\n",
    "    \n",
    "    # 2. Process ExplanatoryNote column\n",
    "    def truncate_at_exclusions(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "            \n",
    "        # Check for both patterns\n",
    "        excludes_split = text.split('Excludes:')\n",
    "        sont_exclus_split = text.split('Sont exclus')\n",
    "        \n",
    "        # If 'Excludes:' is found, use that split\n",
    "        if len(excludes_split) > 1:\n",
    "            return excludes_split[0].strip()\n",
    "        # If 'Sont exclus' is found, use that split\n",
    "        elif len(sont_exclus_split) > 1:\n",
    "            return sont_exclus_split[0].strip()\n",
    "        # If neither pattern is found, return the original text\n",
    "        return text.strip()\n",
    "    \n",
    "    result_df['ExplanatoryNote'] = result_df['ExplanatoryNote'].apply(truncate_at_exclusions)\n",
    "    \n",
    "    # 3. Remove classification markers from Description\n",
    "    markers_pattern = r'\\s*\\((ND|SD|S|D)\\)'\n",
    "    result_df['Description'] = result_df['Description'].str.replace(markers_pattern, '', regex=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level3_en = process_classes(data_en)\n",
    "level3_fr = process_classes(data_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_es[\"Description\"] = data_es[\"Title\"]\n",
    "data_es[\"ExplanatoryNote\"] = data_es[\"ExplanatoryNoteInclusion\"]\n",
    "\n",
    "data_es = data_es.drop(columns=[\"Title\", \"ExplanatoryNoteInclusion\"], axis=1)\n",
    "level3_es = process_classes(data_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([level3_en, level3_fr, level3_es], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize PydanticAI agent to structure information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"mistral-large-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralModel(model_name=llm_model, api_key=config.get(\"MISTRAL_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=(\n",
    "        'You are an expert data curator. You will receive a string of text '\n",
    "        'with examples of items to be included into a certain classification. '\n",
    "        'Your task is to transform this text into a list of self-explainig '\n",
    "        'items descriptions, exploding the examples to the most granular level you can '\n",
    "        'identify in the text. '\n",
    "        'Each item description should contain all necessary information for classification '\n",
    "        'as provided in the original text. Prefer complete descriptions rather than single words. '\n",
    "        'If possible, avoid the use of ambiguous or generic terms such as `other` or `miscellaneous`. '\n",
    "        'Your output should be in the same language as the input text. '\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run calls to the Agent to extract and format information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"ExplanatoryNote\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = \"{}. {}\".format(item.get(\"Description\"), item.get(\"ExplanatoryNote\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"Description\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"Description\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results and failed calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"ExplanatoryNote\"]).to_csv(\n",
    "    \"results/coicop1999_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop1999_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.3 on Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GROQ_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=(\n",
    "        'You are an expert data curator. You will receive a string of text '\n",
    "        'with examples of items to be included into a certain classification. '\n",
    "        'Your task is to transform this text into a list of self-explainig '\n",
    "        'items descriptions, exploding the examples to the most granular level you can '\n",
    "        'identify in the text. '\n",
    "        'Each item description should contain all necessary information for classification '\n",
    "        'as provided in the original text. Prefer complete descriptions rather than single words. '\n",
    "        'If possible, avoid the use of ambiguous or generic terms such as `other` or `miscellaneous`. '\n",
    "        'Your output should be in the same language as the input text. '\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"ExplanatoryNote\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = \"{}. {}\".format(item.get(\"Description\"), item.get(\"ExplanatoryNote\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"Description\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"Description\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"ExplanatoryNote\"]).to_csv(\n",
    "    \"results/coicop1999_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop1999_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek on Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"deepseek-r1-distill-llama-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroqModel(\n",
    "    model_name=llm_model, \n",
    "    api_key=config.get(\"GROQ_API_KEY\"))\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    retries=3,\n",
    "    result_type=CoicopDetails,\n",
    "    system_prompt=(\n",
    "        'You are an expert data curator. You will receive a string of text '\n",
    "        'with examples of items to be included into a certain classification. '\n",
    "        'Your task is to transform this text into a list of self-explainig '\n",
    "        'items descriptions, exploding the examples to the most granular level you can '\n",
    "        'identify in the text. '\n",
    "        'Each item description should contain all necessary information for classification '\n",
    "        'as provided in the original text. Prefer complete descriptions rather than single words. '\n",
    "        'If possible, avoid the use of ambiguous or generic terms such as `other` or `miscellaneous`. '\n",
    "        'Your output should be in the same language as the input text. '\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"ExplanatoryNote\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        prompt = \"{}. {}\".format(item.get(\"Description\"), item.get(\"ExplanatoryNote\"))\n",
    "        agent_result = agent.run_sync(prompt, model_settings={'temperature': 0.0})\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"Description\")] + agent_result.data.model_dump().get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"Description\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"ExplanatoryNote\"]).to_csv(\n",
    "    \"results/coicop1999_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop1999_{}_{}.csv\".format(\n",
    "        llm_model,\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = [f for f in os.listdir(\"results/\") if f.startswith(\"coicop1999\") and f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for f in result_files:\n",
    "    temp_df = pd.read_csv(os.path.join(\"results\", f))\n",
    "    results_list.append(temp_df)\n",
    "\n",
    "results_df = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Code\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to lowercase and remove all \"other\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "results_df[\"Description\"] = results_df[\"Description\"].str.lower()\n",
    "# Remove duplicates\n",
    "results_df = results_df.drop_duplicates(ignore_index=True)\n",
    "# Remove items with \"other\" or \"miscellaneous\"\n",
    "results_df = results_df[~results_df[\"Description\"].str.contains(\"other|miscellaneous\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Code\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\n",
    "    \"results/consolidated_coicop1999_{}.csv\".format(datetime.now().strftime(\"%Y-%m-%d\")), \n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
