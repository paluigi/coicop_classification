{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring COICOP 2018 information for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COICOP 2018 classification contains unstructured information for its most detailed level of classification. This notebook leverages LLMs in order to structure this information in a format which can be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time # Add delays because of free API rate limits\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "from unidecode import unidecode\n",
    "\n",
    "from beeai_framework.adapters.watsonx.backend.chat import WatsonxChatModel\n",
    "from pydantic import BaseModel, Field\n",
    "from beeai_framework.backend.message import UserMessage, SystemMessage\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoicopDetails(BaseModel):\n",
    "    examples: List[str] = Field(description=\"List of products or services in the subclass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = [\"code\",\"title\",\"intro\", \"includes\", \"alsoIncludes\", \"excludes\"]\n",
    "\n",
    "data_df = pd.read_excel(\n",
    "    \"coicop_2018/COICOP_2018_English_structure_edited.xlsx\", \n",
    "    usecols=usecols,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subclasses(df):\n",
    "    \"\"\"\n",
    "    Process DataFrame with the following operations:\n",
    "    1. Filter rows where Code contains exactly 3 dots\n",
    "    2. Combine description columns\n",
    "    3. Remove classification markers from Description\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # 1. Select rows where Code has exactly 3 dots\n",
    "    result_df = result_df[result_df['code'].str.count(r'\\.') == 3]\n",
    "    \n",
    "    # 2. Combine description columns\n",
    "    columns_to_concat = [\"intro\", \"includes\", \"alsoIncludes\"]\n",
    "    \n",
    "    def concatenate_non_nan_columns(row):\n",
    "        # Filter out NaN values and convert to string\n",
    "        non_nan_values = [str(row[col]) for col in columns_to_concat if pd.notna(row[col])]\n",
    "        return ' \\n '.join(non_nan_values) if non_nan_values else ''\n",
    "    \n",
    "    result_df['description'] = result_df.apply(concatenate_non_nan_columns, axis=1)\n",
    "    # Fix encoding issues\n",
    "    result_df['description'] = result_df['description'].apply(fix_text)\n",
    "    result_df['description'] = result_df['description'].apply(unidecode)\n",
    "    result_df['description'] = result_df['description'].str.replace(\"_x000D_\", \" \")\n",
    "\n",
    "    # Fix the exclusion column\n",
    "    result_df['excludes'] = result_df['excludes'].fillna('')\n",
    "    result_df['excludes'] = result_df['excludes'].apply(fix_text)\n",
    "    result_df['excludes'] = result_df['excludes'].apply(unidecode)\n",
    "    result_df['excludes'] = result_df['excludes'].str.replace(\"_x000D_\", \" \")\n",
    "        \n",
    "    # 3. Remove classification markers from Description\n",
    "    markers_pattern = r'\\s*\\((ND|SD|S|D)\\)'\n",
    "    result_df['title'] = result_df['title'].str.replace(markers_pattern, '', regex=True)\n",
    "    \n",
    "    return result_df[[\"code\", \"title\", \"description\", \"excludes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subsubclasses(df):\n",
    "    \"\"\"\n",
    "    Process DataFrame with the following operations:\n",
    "    1. Filter rows where Code contains exactly 4 dots\n",
    "    2. Combine description columns\n",
    "    3. Remove classification markers from Description\n",
    "    4. censor the code to subclass level\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # 1. Select rows where Code has exactly 3 dots\n",
    "    result_df = result_df[result_df['code'].str.count(r'\\.') == 4]\n",
    "    \n",
    "    # 2. Combine description columns\n",
    "    columns_to_concat = [\"intro\", \"includes\", \"alsoIncludes\"]\n",
    "    \n",
    "    def concatenate_non_nan_columns(row):\n",
    "        # Filter out NaN values and convert to string\n",
    "        non_nan_values = [str(row[col]) for col in columns_to_concat if pd.notna(row[col])]\n",
    "        return ' \\n '.join(non_nan_values) if non_nan_values else ''\n",
    "    \n",
    "    result_df['description'] = result_df.apply(concatenate_non_nan_columns, axis=1)\n",
    "    # Fix encoding issues\n",
    "    result_df['description'] = result_df['description'].apply(fix_text)\n",
    "    result_df['description'] = result_df['description'].apply(unidecode)\n",
    "    result_df['description'] = result_df['description'].str.replace(\"_x000D_\", \" \")\n",
    "\n",
    "    # Fix the exclusion column\n",
    "    result_df['excludes'] = result_df['excludes'].fillna('')\n",
    "    result_df['excludes'] = result_df['excludes'].apply(fix_text)\n",
    "    result_df['excludes'] = result_df['excludes'].apply(unidecode)\n",
    "    result_df['excludes'] = result_df['excludes'].str.replace(\"_x000D_\", \" \")\n",
    "        \n",
    "    # 3. Remove classification markers from Description\n",
    "    markers_pattern = r'\\s*\\((ND|SD|S|D)\\)'\n",
    "    result_df['title'] = result_df['title'].str.replace(markers_pattern, '', regex=True)\n",
    "\n",
    "    # 4. censor the code to subclass level\n",
    "    result_df['code'] = result_df['code'].str[:8]\n",
    "    \n",
    "    return result_df[[\"code\", \"title\", \"description\", \"excludes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsub_df = process_subsubclasses(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = process_subclasses(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([sub_df, subsub_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove divisions 14 and 15, as they do not add further information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[~data_df[\"code\"].str.startswith((\"14\", \"15\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"ou are an expert data curator. When provided with a subclass name and its inclusion/exclusion description:\n",
    "\n",
    "1. Identify specific products/services that belong in this subclass based on the inclusion description. Use exclusions only to understand boundaries.\n",
    "\n",
    "2. Generate a list of specific product/service names that:\n",
    "   - Belong within the defined subclass\n",
    "   - Represent specific items, not categories\n",
    "   - Cover the full range of inclusions\n",
    "\n",
    "3. Make each name semantically unique by:\n",
    "   - Avoiding generic terms (\"other,\" \"miscellaneous\")\n",
    "   - Minimizing word overlap between entries\n",
    "   - Using varied terminology\n",
    "\n",
    "4. Output in the same language as input, preserving industry terminology.\n",
    "\n",
    "5. Format as a simple list of product/service names.\n",
    "\"\"\"\n",
    "\n",
    "subclass_prompt = \"\"\"Subclass title: {title}\n",
    "\n",
    "Inclusion note: {description}\n",
    "\n",
    "Exclusion note: {excludes}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.3 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"meta-llama/llama-3-3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxChatModel(\n",
    "    model_id=llm_model,\n",
    "    project_id=config.get(\"GRANITE_PROJECT\"),\n",
    "    api_key=config.get(\"GRANITE_API_KEY\"),\n",
    "    api_base=config.get(\"GRANITE_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def watson_structure(llm: WatsonxChatModel, system_prompt: str, subclass_prompt: str, item: dict) -> None:\n",
    "    system_message = SystemMessage(system_prompt)\n",
    "    user_message = UserMessage(subclass_prompt.format(\n",
    "        title=item.get(\"title\"), \n",
    "        description=item.get(\"description\"), \n",
    "        excludes=item.get(\"excludes\")))\n",
    "    response = await llm.create_structure(\n",
    "        {\n",
    "            \"schema\": CoicopDetails,\n",
    "            \"messages\": [system_message, user_message],\n",
    "        }\n",
    "    )\n",
    "    return response.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        agent_result = await watson_structure(llm, system_prompt, subclass_prompt, item)\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        \"llama-3-3-70b-instruct\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        \"llama-3-3-70b-instruct\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"mistralai/mistral-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxChatModel(\n",
    "    model_id=llm_model,\n",
    "    project_id=config.get(\"GRANITE_PROJECT\"),\n",
    "    api_key=config.get(\"GRANITE_API_KEY\"),\n",
    "    api_base=config.get(\"GRANITE_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def watson_structure(llm: WatsonxChatModel, system_prompt: str, subclass_prompt: str, item: dict) -> None:\n",
    "    system_message = SystemMessage(system_prompt)\n",
    "    user_message = UserMessage(subclass_prompt.format(\n",
    "        title=item.get(\"title\"), \n",
    "        description=item.get(\"description\"), \n",
    "        excludes=item.get(\"excludes\")))\n",
    "    response = await llm.create_structure(\n",
    "        {\n",
    "            \"schema\": CoicopDetails,\n",
    "            \"messages\": [system_message, user_message],\n",
    "        }\n",
    "    )\n",
    "    return response.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        agent_result = await watson_structure(llm, system_prompt, subclass_prompt, item)\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        \"mistral-large\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        \"mistral-large\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Granite 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"ibm/granite-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxChatModel(\n",
    "    model_id=llm_model,\n",
    "    project_id=config.get(\"GRANITE_PROJECT\"),\n",
    "    api_key=config.get(\"GRANITE_API_KEY\"),\n",
    "    api_base=config.get(\"GRANITE_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def watson_structure(llm: WatsonxChatModel, system_prompt: str, subclass_prompt: str, item: dict) -> None:\n",
    "    system_message = SystemMessage(system_prompt)\n",
    "    user_message = UserMessage(subclass_prompt.format(\n",
    "        title=item.get(\"title\"), \n",
    "        description=item.get(\"description\"), \n",
    "        excludes=item.get(\"excludes\")))\n",
    "    response = await llm.create_structure(\n",
    "        {\n",
    "            \"schema\": CoicopDetails,\n",
    "            \"messages\": [system_message, user_message],\n",
    "        }\n",
    "    )\n",
    "    return response.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_calls = []\n",
    "for i, item in enumerate(data_dict):\n",
    "    # Print every 20 items to show progress\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Processing item {i+1} out of {len(data_dict)}\")\n",
    "    # Add switch to skip none items\n",
    "    if item.get(\"description\") is None:\n",
    "        # No information to parse, just append existing item\n",
    "        results.append(item)\n",
    "        continue\n",
    "    # Time delay to respect API rate limits\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        agent_result = await watson_structure(llm, system_prompt, subclass_prompt, item)\n",
    "        # Add to results all examples, including the original class name...list concatenation\n",
    "        for ex in [item.get(\"title\")] + agent_result.get(\"examples\"):\n",
    "            # the new \"Description\" is inserted at the end, so it overwrites the original one\n",
    "            results.append({**item, \"title\": ex})\n",
    "    except Exception as e:\n",
    "        failed_calls.append(item)\n",
    "        print(f\"Error processing item {i+1} out of {len(data_dict)}\")\n",
    "        print(item)\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.drop(columns=[\"description\", \"excludes\"]).to_csv(\n",
    "    \"results/coicop2018_{}_{}.csv\".format(\n",
    "        \"granite-3-8b-instruct\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = pd.DataFrame(failed_calls)\n",
    "failed_df.to_csv(\n",
    "    \"results/failed_coicop2018_{}_{}.csv\".format(\n",
    "        \"granite-3-8b-instruct\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
